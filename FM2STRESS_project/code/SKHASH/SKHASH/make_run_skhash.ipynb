{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Focal mechanism calculation using `SKHASH` file format\n",
    "\n",
    "**Make P-polarity input file i.e., `pol_consensus.csv` file.**\n",
    "\n",
    "columns:\\\n",
    "        `event_id,event_id2,station,location,channel,p_polarity,origin_latitude,origin_longitude,origin_depth_km`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_skhash_polarity_file(data_path, catalog_path, output_path):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function creates a polarity file for SKHASH format input data\n",
    "    input: \n",
    "        data_path: path to the data folder that includes event folders i.e., folder names are event_ids\n",
    "        and picks files from pyrocko..\n",
    "        catalog_path: path to the catalog file\n",
    "    output:\n",
    "        pol_consensus.csv file written to ouput folder\n",
    "    \"\"\"\n",
    "\n",
    "    import os\n",
    "    import glob\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    # create an empty dataframe with column names [event_id,event_id2,station,location,channel,p_polarity,origin_latitude,origin_longitude,origin_depth_km]\n",
    "    pol_df = pd.DataFrame()\n",
    "\n",
    "    # get a list of folderes only in the data_path i.e. event_ids\n",
    "    event_ids = [os.path.basename(path) for path in glob.glob(f\"{data_path}/*\") if os.path.isdir(path)]\n",
    "\n",
    "    # loop over each event_id\n",
    "    for i, event_id in enumerate(event_ids):\n",
    "\n",
    "        # read the even_eventid_picks.csv file\n",
    "        # only column columns 4,8,9\n",
    "        # col4 has NN.SSSSS.LL.CCC format (network.station.location.channel)\n",
    "\n",
    "        # check if polarity file has been created for this event_id\n",
    "        if not os.path.exists(f\"{data_path}/{event_id}/event_{event_id}_picks.txt\"):\n",
    "            print(f\"{'='*10} Skipping event_id {event_id} as polarity file does not exist\")\n",
    "            continue\n",
    "\n",
    "        picks_df = pd.read_csv(\n",
    "            f\"{data_path}/{event_id}/event_{event_id}_picks.txt\",\n",
    "            skiprows=1,         # skip the first row\n",
    "            sep='\\s+',          # delimiter\n",
    "            header=None,        # no header\n",
    "            usecols=[4,8,9],    # only read columns 4,8,9,\n",
    "            names=['stns', 'phase', 'polarity']\n",
    "            )\n",
    "        # get 'origin_latitude','origin_longitude','origin_depth_km' from original catalog file\n",
    "        ## or we can also use best_location file we created using misfit calculation\n",
    "        catalog_df = pd.read_csv(catalog_path, sep=',', header=0, \n",
    "                                usecols=['latitude','longitude','depth', 'id'],\n",
    "                                )\n",
    "        catalog_df = catalog_df[catalog_df['id'] == event_id]\n",
    "\n",
    "        # create a temporary dataframe with columns [event_id,event_id2,station,location,channel,p_polarity]\n",
    "        temp_df = pd.DataFrame(columns=['event_id','event_id2','station','location','channel','p_polarity'])\n",
    "        \n",
    "        temp_df['station'] = picks_df['stns'].str.split('.').str[1]\n",
    "        temp_df['location'] = picks_df['stns'].str.split('.').str[2]\n",
    "        temp_df['location'].replace('', '--', inplace=True)\n",
    "\n",
    "        temp_df['channel'] = picks_df['stns'].str.split('.').str[3]\n",
    "        temp_df['p_polarity'] = picks_df['polarity']\n",
    "        temp_df['event_id'] = event_id\n",
    "        temp_df['event_id2'] = i+1\n",
    "        temp_df['origin_latitude'] = format(catalog_df['latitude'].values[0], '.5f')\n",
    "        temp_df['origin_longitude'] = format(catalog_df['longitude'].values[0], '.5f')\n",
    "        temp_df['origin_depth_km'] = catalog_df['depth'].values[0]\n",
    "\n",
    "        # append the temp_df to pol_df\n",
    "        pol_df = pd.concat([pol_df, temp_df], ignore_index=True)\n",
    "\n",
    "        # drop rows with nan values in p_polarity column\n",
    "        pol_df.dropna(subset=['p_polarity'], inplace=True)\n",
    "        \n",
    "\n",
    "    # write the pol_df dataframe to a csv file\n",
    "    print(f\"{'='*10} Writing the `pol_consensus.csv` file to {output_path}\")\n",
    "    pol_df.to_csv(f'{output_path}/pol_consensus.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the station file\n",
    "columns: \n",
    "- `station,location,channel,latitude,longitude,elevation` \\\n",
    "for this we will read the event_inventory.txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_skhash_station_file(\n",
    "    output_path,\n",
    "    client_list = ['IRIS', 'NCEDC'], \n",
    "    starttime = \"2008-01-01\", \n",
    "    endtime = \"2023-01-01\",\n",
    "    region = [-128, -122.5, 39, 42],\n",
    "    channel = 'HH*,BH*,HN*,EH*',\n",
    "    given_inventory = None\n",
    "    ):\n",
    "\n",
    "    \"\"\" \n",
    "    This function creates a station file for SKHASH format input data.\n",
    "    No need to input a inventory file it will download the inventory from the client_list\n",
    "    input:\n",
    "        output_path: path to the output folder (mandatory)\n",
    "        client_list: list of clients to download the inventory from (default: ['IRIS', 'NCEDC'])\n",
    "        starttime: start time of the inventory (default: \"2008-01-01\")\n",
    "        endtime: end time of the inventory (default: \"2023-01-01\")\n",
    "        region: region of the inventory (default: [-128, -122.5, 39, 42])\n",
    "        channel: channel of the inventory (default: 'HH*,BH*,HN*,EH*')\n",
    "        given_inventory: path to the inventory file (default: None)\n",
    "    output:\n",
    "        station.csv file written to ouput folder\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    import obspy\n",
    "    from obspy import read, UTCDateTime, read_inventory, Inventory\n",
    "    from obspy.clients.fdsn import Client\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import os\n",
    "    \n",
    "    if given_inventory == None:    \n",
    "        # get station inventory\n",
    "\n",
    "        starttime = UTCDateTime(starttime)\n",
    "        endtime = UTCDateTime(endtime)\n",
    "\n",
    "        minlongitude = region[0]\n",
    "        maxlongitude = region[1]\n",
    "        minlatitude = region[2]\n",
    "        maxlatitude = region[3]\n",
    "\n",
    "        merged_inventory = Inventory()\n",
    "        for iclient in client_list:\n",
    "            print(iclient)\n",
    "            client = Client(iclient)\n",
    "            try:\n",
    "                try:\n",
    "                    inv = client.get_stations(\n",
    "                        starttime = starttime,\n",
    "                        endtime = endtime,\n",
    "                        minlatitude = minlatitude,\n",
    "                        maxlatitude = maxlatitude,\n",
    "                        minlongitude = minlongitude,\n",
    "                        maxlongitude = maxlongitude,\n",
    "                        channel = channel,\n",
    "                        level = 'response'\n",
    "                    )\n",
    "                    merged_inventory.networks.extend(inv.networks)\n",
    "                except:\n",
    "                    pass\n",
    "            except:\n",
    "                print(f\"Failed to get inventory from {iclient}\")\n",
    "                continue\n",
    "\n",
    "        # temorarily write merged inventory to a file\n",
    "        merged_inventory.write(\"mtj_merged_stations_temp.txt\", format=\"STATIONTXT\")\n",
    "        given_inventory = \"mtj_merged_stations_temp.txt\"\n",
    "\n",
    "    else:\n",
    "        print(f\"Using the provided merged inventory file: {given_inventory}\")\n",
    "        pass\n",
    "\n",
    "    # read the merged inventory file\n",
    "    inv_df = pd.read_csv(given_inventory, sep='|', skiprows=1,\n",
    "                        usecols=[1, 2, 3, 4, 5, 6],\n",
    "                        names=['station','location','channel','latitude','longitude','elevation'],\n",
    "                        dtype={'station': str,'location': str, 'channel': str, 'latitude': float, 'longitude': float,\n",
    "                                'elevation': float},    \n",
    "                        )\n",
    "    # replace the empty location with '--'\n",
    "    # inv_df['location'].fillna('--', inplace=True)\n",
    "    inv_df['location'].replace(pd.NA, '--', inplace=True)\n",
    "\n",
    "    # sort by station and write the station file\n",
    "    print(f\"{'='*10} Writing the `station.csv` file to {output_path}\")\n",
    "    inv_df.sort_values(by=['station']).reset_index(drop=True)\n",
    "    inv_df.drop_duplicates(subset=['station', 'location', 'channel'], keep='first', inplace=True) \n",
    "    inv_df.to_csv(f'{output_path}/station.csv', index=False)\n",
    "\n",
    "    # delete the temporary file\n",
    "    if os.path.exists(\"mtj_merged_stations_temp.txt\"):\n",
    "        os.remove(\"mtj_merged_stations_temp.txt\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the reverse file\n",
    "name: `reverse.csv` \\\n",
    "columns: `station,location,channel,start_time,end_time`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_skhash_reverse_file(polarity_file, output_path):\n",
    "    \"\"\"\n",
    "    make sure the polarity_file has been created using the make_skhash_polarity_file function\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    \n",
    "    df = pd.read_csv(polarity_file, header=0)\n",
    "    \n",
    "    reverse_df = pd.DataFrame()\n",
    "    reverse_df['station'], reverse_df['location'], reverse_df['channel'] = df['station'], df['location'], df['channel']\n",
    "    reverse_df.drop_duplicates(subset=['station'], inplace=True)\n",
    "\n",
    "    # put dummy start and end time\n",
    "    reverse_df['start_time'] = \"1900-01-01\"\n",
    "    reverse_df['end_time'] = \"2200-01-01\"\n",
    "\n",
    "    print(f\"{'='*10} Writing the `reverse.txt` file to {output_path}\")\n",
    "    reverse_df.to_csv(f'{output_path}/reverse.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data/eq_data\n",
      "========== Writing the `pol_consensus.csv` file to ./examples/maacama_SKHASH_MTJ/IN/\n",
      "Using the provided merged inventory file: ./examples/maacama_SKHASH_MTJ/IN/mtj_merged_stations.txt\n",
      "========== Writing the `station.csv` file to ./examples/maacama_SKHASH_MTJ/IN/\n",
      "========== Writing the `reverse.txt` file to ./examples/maacama_SKHASH_MTJ/IN/\n"
     ]
    }
   ],
   "source": [
    "# data_path = f\"./examples/maacama_SKHASH_MTJ/IN/\"\n",
    "final_project_folder = \"../../data\"\n",
    "data_path = f\"{final_project_folder}/eq_data\"\n",
    "catalog_path = f\"{final_project_folder}/above_slab_eq_0.2_grid.csv\"\n",
    "output_path = f\"./examples/maacama_SKHASH_MTJ/IN/\"\n",
    "\n",
    "print(f\"{data_path}\")\n",
    "\n",
    "make_skhash_polarity_file(data_path, catalog_path, output_path)\n",
    "\n",
    "make_skhash_station_file(output_path, given_inventory = './examples/maacama_SKHASH_MTJ/IN/mtj_merged_stations.txt')\n",
    "\n",
    "make_skhash_reverse_file(f\"{output_path}/pol_consensus.csv\", output_path)\n",
    "\n",
    "f = \"../../data/eq_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "controlfile : examples/maacama_SKHASH_MTJ/control_file.txt\n",
      "========================\n",
      "SKHASH v0.1 (2023-12-15)\n",
      "========================\n",
      "Control file: examples/maacama_SKHASH_MTJ/control_file.txt\n",
      "Creating lookup table (0/0): examples/velocity_models_MTJ/vz.MTJ.txt\n",
      "\tCreated table.\n",
      "Computing mechanisms in serial...\n",
      "0 / 17\t(nc40216664)\n",
      "\tS: 303.4656   D: 48.5403   R: -72.4056   U: 28.3596   Q: C\n",
      "\tRuntime: 0.17 sec\n",
      "1 / 17\t(nc51207076)\n",
      "\tMaximum azimuthal gap (150.498) > max_agap (90.0). Skipping.\n",
      "2 / 17\t(nc71100926)\n",
      "\tMaximum azimuthal gap (128.65) > max_agap (90.0). Skipping.\n",
      "3 / 17\t(nc71349716)\n",
      "\tS: 129.6605   D: 76.8136   R: 179.882   U: 41.8478   Q: D\n",
      "\tRuntime: 0.11 sec\n",
      "4 / 17\t(nc71544046)\n",
      "\tMaximum azimuthal gap (268.917) > max_agap (90.0). Skipping.\n",
      "5 / 17\t(nc71872550)\n",
      "\tMaximum azimuthal gap (99.495) > max_agap (90.0). Skipping.\n",
      "6 / 17\t(nc72006040)\n",
      "\tMaximum azimuthal gap (101.864) > max_agap (90.0). Skipping.\n",
      "7 / 17\t(nc72086051)\n",
      "\tMaximum azimuthal gap (262.12) > max_agap (90.0). Skipping.\n",
      "8 / 17\t(nc72322146)\n",
      "\tS: 126.7955   D: 59.7598   R: -174.2306   U: 25.2952   Q: D\n",
      "\tRuntime: 0.31 sec\n",
      "9 / 17\t(nc72946846)\n",
      "\tS: 218.7575   D: 70.7078   R: 137.1514   U: 43.6954   Q: D\n",
      "\tRuntime: 0.16 sec\n",
      "10 / 17\t(nc73024816)\n",
      "\tS: 332.8421   D: 60.7239   R: -141.0128   U: 37.963   Q: C\n",
      "\tRuntime: 0.15 sec\n",
      "11 / 17\t(nc73118836)\n",
      "\tMaximum azimuthal gap (91.37) > max_agap (90.0). Skipping.\n",
      "12 / 17\t(nc73139111)\n",
      "\tMaximum azimuthal gap (256.086) > max_agap (90.0). Skipping.\n",
      "13 / 17\t(nc73139526)\n",
      "\tMaximum azimuthal gap (134.507) > max_agap (90.0). Skipping.\n",
      "14 / 17\t(nc73201181)\n",
      "\tS: 105.4631   D: 87.3573   R: 177.6072   U: 21.549   Q: C\n",
      "\tRuntime: 0.29 sec\n",
      "15 / 17\t(nc73311641)\n",
      "\tS: 160.9533   D: 40.9092   R: -116.0621   U: 21.3298   Q: B\n",
      "\tRuntime: 0.21 sec\n",
      "16 / 17\t(nc73629686)\n",
      "\tS: 38.8259   D: 78.0968   R: -157.2264   U: 43.508   Q: C\n",
      "\tRuntime: 0.22 sec\n",
      "17 / 17\t(nc73783911)\n",
      "\tMaximum azimuthal gap (96.262) > max_agap (90.0). Skipping.\n",
      "Mech computation runtime: 1.65 sec\n",
      "Total runtime: 1.97 sec\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.system('python3 SKHASH.py examples/maacama_SKHASH_MTJ/control_file.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "obspy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
