{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the inventory data for the following criterias\n",
    "Select an earthquake event (by id).\\\n",
    "Get the station inventory within the study area which recorded this event.\\\n",
    "Use this station inventory data to download earthquake data (`.mseed`) and station data (`.txt` or `.xml` format).\\\n",
    "3-channel data for earthquakes are along x, y and z axis. Usually named as HHE, HHN and HHZ or HH1, HH2, HHZ.\\\n",
    "I want to select the following channels in order. If the 1st one is available, get that one only and stop. If not search for the 2nd one and so on.\n",
    "1. HH*\n",
    "2. BH*\n",
    "3. HN*\n",
    "4. EH* \\\n",
    "Lastly, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from obspy.clients.fdsn import Client\n",
    "from obspy import UTCDateTime, read_inventory, Inventory, read, Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number grids : 47\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "# Read earthquake data\n",
    "eqdf = pd.read_csv(\"../data/above_slab_eq_0.2_grid.csv\", parse_dates=[\"time\"])\n",
    "eqdf = eqdf[eqdf.mag >= 2.5].reset_index(drop=True)\n",
    "\n",
    "# Extract unique grid codes and sort them\n",
    "unique_grid_codes = np.sort(eqdf['grid_code'].unique())\n",
    "print(f\"Number grids : {len(unique_grid_codes)}\")\n",
    "\n",
    "# create an empty dataframe to store the selected events\n",
    "selected_eq = pd.DataFrame(columns=eqdf.columns)\n",
    "\n",
    "# Loop through each grid code\n",
    "for i in range(len(unique_grid_codes)):\n",
    "\n",
    "    # select events in the grid\n",
    "    temp_df = eqdf[eqdf.grid_code == unique_grid_codes[i]].reset_index(drop=True) # select events in the grid\n",
    "    # sort the events by magnitude\n",
    "    temp_df = temp_df.sort_values(by='mag', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # Check if there are events in the grid\n",
    "    if len(temp_df) > 0:\n",
    "        # print(temp_df[['id', 'mag', 'time']])\n",
    "        # check the depth values if 0 or 5 or 10 km fixed depth, select the next event\n",
    "        for i, row in temp_df.iterrows():\n",
    "            if row['depth'] == 0 or row['depth'] == 5 or row['depth'] == 10:\n",
    "                continue\n",
    "            else:\n",
    "                selected_eq = pd.concat([selected_eq, row.to_frame().T], ignore_index=True)\n",
    "                break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a data folder to store all data in the same folder\n",
    "data_path = \"../data/eq_data/all_data\"\n",
    "os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "# Loop through each grid code\n",
    "for i, row in selected_eq.iterrows():\n",
    "    # get the details of the event\n",
    "    event_id = row['id']\n",
    "\n",
    "    print(f\"{'#'*5} Selected event: {event_id}, mag: {row['mag']} in grid {unique_grid_codes[i]}\")\n",
    "\n",
    "    # if an event is already downloaded, skip\n",
    "    if os.path.exists(f\"{data_path}/{event_id}_station_inventory.txt\"):\n",
    "        print(f\"{'#'*10} Event {event_id} already downloaded. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # Get event time\n",
    "    event_time = UTCDateTime(pd.to_datetime(row.time))\n",
    "    starttime = event_time - 40\n",
    "    endtime = event_time + 120\n",
    "\n",
    "    # define the datacenters and channel list\n",
    "    client_list = ['IRIS', 'NCEDC', 'SCEDC']\n",
    "    channel_list = 'HH*,BH*,HN*,EH*' # select broadband and high sample rate channels\n",
    "\n",
    "    # Create a folder for the event\n",
    "    output_folder = data_path\n",
    "\n",
    "    merged_inventory = Inventory()\n",
    "\n",
    "    # Loop through each client (IRIS, NCEDC, SCEDC data centers)\n",
    "    for client_name in client_list:\n",
    "        client = Client(client_name, debug=False, timeout=60)\n",
    "        try:\n",
    "            inv = client.get_stations(\n",
    "                network=\"*\",\n",
    "                station=\"*\",\n",
    "                location=\"*\",\n",
    "                channel=channel_list,\n",
    "                starttime=starttime,\n",
    "                endtime=endtime,\n",
    "                level=\"channel\",\n",
    "                minlatitude=39,\n",
    "                maxlatitude=42,\n",
    "                minlongitude=-128,\n",
    "                maxlongitude=-122.5, # extend the area by 0.5 deg\n",
    "            )\n",
    "            merged_inventory.networks.extend(inv.networks)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching data from {client_name}: {e}\")    \n",
    "\n",
    "    # write the whole inventory to a file\n",
    "    # merged_inventory = merged_inventory.remove(network=\"SY\")\n",
    "    # merged_inventory.write(f\"{data_path}/{event_id}_station_inventory.xml\", format=\"STATIONXML\")\n",
    "    merged_inventory.write(f\"{data_path}/{event_id}_station_inventory.txt\", format=\"STATIONTXT\")\n",
    "    # merged_inventory.plot(projection=\"local\", resolution=\"i\", label=False, show=False);\n",
    "\n",
    "    # break # test with only one grid\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the inventory files already downloaded \n",
    "# And download `.mseed` and station_data (`.xml & .txt`)\n",
    "\n",
    "Here I will read the inventory file which contain details about all the stations that recorded a particular earthquake event. \\\n",
    "From that inventory file I will get all the necessary informations I need to download the seismic data (a numpy timeseries in `.mseed` format). \\\n",
    "I will also download the metadata for that record in `.xml & .txt` formats.\\\n",
    "\n",
    "This process will use `multiprocessing.Pool.imap_unordered` module for paraller processing of the download.\\\n",
    "For the code see `./code/my_funcs/get_waveforms_parallel_v3.py` where I defined the download fuction combined with parallel processing. \\\n",
    "This significantly improves the runtime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading events:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event nc73783911 already downloaded. Skipping...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# reload the module to get the latest changes\n",
    "import sys\n",
    "sys.path.append('./my_funcs')\n",
    "# %load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "import glob\n",
    "import os \n",
    "from tqdm.notebook import tqdm\n",
    "# import all the `get_waveforms` function\n",
    "from my_funcs.get_waveforms_parallel_v3 import *\n",
    "\n",
    "# define the client list i.e. the data centers to download data from\n",
    "client_list = ['NCEDC', 'IRIS'] #, 'SCEDC']\n",
    "\n",
    "# get a list of all the event id folders\n",
    "event_paths = glob.glob(\"../data/eq_data/*\")\n",
    "event_ids = [os.path.basename(path) for path in event_paths if os.path.isdir(path)] # get the event ids from folder names\n",
    "\n",
    "# Read earthquake data\n",
    "eqdf = pd.read_csv(\"../data/above_slab_eq_df.csv\", parse_dates=[\"time\"])\n",
    "\n",
    "# define the priority channels\n",
    "priority_channels = ['HH*', 'BH*', 'HN*', 'EH*']\n",
    "\n",
    "event_ids = ['nc73783911'] # test with one event #################################### change it ####################\n",
    "\n",
    "# Create tqdm instance with the total number of iterations\n",
    "progress_bar = tqdm(total=len(event_ids), desc=\"Downloading events\")\n",
    "\n",
    "# loop through each event id and download the data\n",
    "for event_id in event_ids:\n",
    "\n",
    "    # define the output folder\n",
    "    output_folder = f\"../data/eq_data/{event_id}/\"\n",
    "\n",
    "    # check if the event data is already downloaded\n",
    "    if os.path.exists(f\"../data/eq_data/{event_id}/event_waveforms.mseed\"):\n",
    "        print(f\"Event {event_id} already downloaded. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    print(f\"{' '*8}Getting data for event {event_id}\")\n",
    "\n",
    "    # Read the inventory\n",
    "    inventory = read_inventory(f\"../data/eq_data/{event_id}/inventory/station_inventory_{event_id}.xml\")\n",
    "\n",
    "    #get the event time, start time and end time\n",
    "    eq = eqdf[eqdf.id == event_id] # get the event details\n",
    "    event_time = UTCDateTime(pd.to_datetime(eq.time.values[0])) # get the event time in UTC format\n",
    "    starttime = event_time - 30 # start time is 30 seconds before the event time\n",
    "    endtime = event_time + 120 # end time is 120 seconds after the event time\n",
    "\n",
    "    # Call the function with the desired parameters\n",
    "    # this will downaload and write the data to a file, to change path, edit the function\n",
    "    get_waveforms_parallel(client_list, inventory, starttime, endtime, output_folder, priority_channels)\n",
    "\n",
    "    # update the progress bar\n",
    "    progress_bar.update(1)\n",
    "\n",
    "    # break # test with only one event\n",
    "\n",
    "# close the progress bar\n",
    "progress_bar.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "obspy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
